<!DOCTYPE html>
<html lang="en">
<head>
    <title>hd_mapping</title>
    <meta property="og:title" content="hd_mapping"/>

    <link rel="stylesheet" type="text/css" href="css/style.css">
</head>

<body>

<!-- Title -->
<br>
<center>
    <span style="font-size:38px">HD Mapping for Automonous Driving</span>
</center>


<!-- Organizations -->
<br>
<table align=center width=900px>
    <tr>
        <td align=center width=100px>
            <center>
                <span style="font-size:20px"><sup>1</sup>MMVC Lab, New York University</span>
            </center>
        </td>
    </tr>
</table>


<!-- Research Statement -->
<center><h1>Research Statement</h1></center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
           In our current era of versatile portable environmental sensors and artificial intelligence (AI) and computer
vision (CV) driven agent (i.e. self-driving vehicles), there has been a significant increase of interests in
the research and development of the autonomous vehicle to improve driving and fuel efficiency to help
controlling the traffic flow and parking problems. One of the key research components that drives the de-
velopment of autonomous vehicle is the computer vision based environmental perception, through which
the autonomous vehicle (equipped with various multi-modal sensors) is guided and enabled to sense and
react to its surrounding immediate environment in order to navigate roadways without human intervention.
More specifically, the perception process often involves a sequence of visual computing subtasks such as
object classification, detection, 3D position estimation, and simultaneous localization and mapping (SLAM).
        </p>
        <p align="justify" style="font-size: 18px">&nbsp;&nbsp;&nbsp;&nbsp;
           Extensive research and development is taking place on different aspects of intelligent autonomous robotic
system design, which enable many companies and research laboratories to work on developing autonomous
vehicles that are capable of navigating in 3D environments, e.g., self-driving cars and self-flying drones.
Over recent years, we have also seen increasing interests among car manufacturers to develop advanced
driver assistance systems (ADAS) that alert drivers to potential problems on the road. However, many challenging issues in autonomy and compute vision still remain in the design of robust intelligent autonomous
vehicles.
        </p>
        <p align="justify" style="font-size: 18px">&nbsp;&nbsp;&nbsp;&nbsp;
           We clearly need to move further beyond current ADAS technology, with a paradigm (i.e. object detection,
classification, pose estimation, and SLAM) shifting towards state of the art deep learning visual computing
techniques that will allow vehicle to gain more autonomy. To this end, the proposed project aims to start
with the development of a robust object detection application suite, named ARBI-DETC (arbitrary objection
detection in unconstrained scene), as a key component of the advanced driver assistance system (ADAS) for
the autonomous vehicle equipped with HD live map. This proposed ARBI-DETC suite will develop both
2D and 3D object detection in parallel with ARBI-DETC 2D for 2D object detection in image and video and
            ARBI-DETC 3D for 3D object detection in real scene. ARBI-DETC application suite
consist of two main elements ARBI-DETC 2D and ARBI-DETC 3D . The first element set includes a collection of various object detection (i.e. text detection, car detection, lane detection and pedestrian detection)
as shown in the figure. The ARBI-DETC application suite begins with the research and development of
arbitrary text detection in unconstrained scene as an independent component named Arbi-Detc 2D Text, and
consequently will further continue on other object detections such as cars, lanes, and pedestrians.
        </p>
    </td>
</tr>
<tr></tr>
</table>


<!-- Research Objectives -->
<hr>
<br>
<center><h1>Research Objectives</h1></center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
            The proposed research project is to develop of a novel unified framework and algorithms for vehicle-centered
real-time environmental awareness by detecting the objects of interests in a vehicles immediate surround-
ings. The approach leverages state-of-the-art techniques from multiple research domains including deep
neural network design of object detection, semantic scene parsing networks, and deep adversarial learning
for security mitigation for the security sensitive application of autonomous vehicles. We aim to complete the
proposed project with the following objectives in three years based on our current development of ArbiText:
        </p>
        <p><b>OBJECTIVE 1: Design deep neural networks for general object detection and semantic scene parsing </b> </p>
        <p><b>OBJECTIVE 2: Design deep adversarial neural networks for security mitigation</b></p>
        <p><b>OBJECTIVE 3: Design deep neural networks for 3D object detections</b></p>
    </td>
</tr>
<tr></tr>
</table>

<!-- Approaches -->
<br>
<hr>
<center><h1>Approaches</h1></center>
<br>
<table align=center width="1024px">
    <tr>
        <td>
            <center>
                <img src="./resources/pro2_2.png" width="900px"/><br>
            </center>
        </td>
    </tr>
    <tr>
        <td>
            <center>
                <span style="font-size:18px"></span>
            </center>
        </td>
    </tr>

    <tr>
        <td colspan='2'>
            <center>
                <img src="./resources/pro2_3.png" width="900px"/><br>
            </center>
        </td>
    </tr>
    <tr>
        <td colspan='2'>
            <center>
                <span style="font-size:18px"></span>
            </center>
        </td>
    </tr>

    <tr>
        <td colspan='2'>
            <center>
                <img src="./resources/pro2_4.png" width="900px"/><br>
            </center>
        </td>
    </tr>
    <tr>
        <td colspan='2'>
            <center>
                <span style="font-size:18px"></span>
            </center>
        </td>
    </tr>
</table>
<! -- Broader Impacts -->
<br>
<hr>
<center>
    <h1>Broader Impacts</h1>
</center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
           AI-CARES uses computer vision to assist visually impaired (VI) individuals in carrying out daily
            activities national health crises like COVID-19. Due to blindness or reduced vision, the VI often
            had a lower degree of independence compared to people with regular vision. In non-pandemic times,
            VIs compensated their visual deficiencies with alternatives to vision in daily lives, like working
            with a guide person to help with grocery shopping or exploring a new environment by touch. Many of
            these compensatory methods have become risk-inducing or mentally stressing due to health and safety
            guidelines during pandemics, such as avoiding contact with object surfaces or maintaining social
            distance. At a high level, the proposed AI-CARES helps VI individuals achieve
            a higher degree of independence by offering a VI AI-powered interactive and informative perceptive
            alternatives to vision so they rely less on touch or physical assistance from another person during
            common activities. Another core part of AI-CARE is its reach-maximizing design philosophy that shapes
            its hardware and technology to be easy to obtain and intuitive to adopt.
        </p>
    </td>
</tr>
<tr></tr>
</table>


<!-- Credit -->
<hr>
<br>
<table style="font-size:14px" align=center>
    <tr>
        <td>
            This webpage template was borrowed from <a href='../../2020/MEPS'>MEPS</a>.
        </td>
    </tr>
</table>

</body>
</html>