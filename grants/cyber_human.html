<!DOCTYPE html>
<html lang="en">
<head>
    <title>cyber_human</title>
    <meta property="og:title" content="cyber_human"/>

    <link rel="stylesheet" type="text/css" href="css/style.css">
</head>

<body>

<!-- Title -->
<br>
<center>
    <span style="font-size:38px">Cyber-Human Interaction Learning to Unpack Activities of Daily Living for the Visually Impaired</span>
</center>


<!-- Organizations -->
<br>
<table align=center width=900px>
    <tr>
        <td align=center width=100px>
            <center>
                <span style="font-size:20px"><sup>1</sup>MMVC Lab, New York University</span>
            </center>
        </td>
    </tr>
</table>



<!-- Research Statement -->
<center><h1>MOTIVATION and GOAL</h1></center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
           The World Health Organization estimates there are approximately 253 million people live with
vision impairment, with 81% of them aged 50 years old and
above, and 7.5% of them aged below 15 years old. Blindness
and low vision result in a host of social, emotional and health-related problems, as increasingly limited vision leads
            increasingly to impaired mobility, often secondary to falls, injuries,
and co-morbidities. The economic impact is estimated to have
approached $3 trillion dollars globally in 2010 with conservative estimates approaching $75 billion dollars per annum in the United States alone. While therapeutic advances are
            in the 'pipeline' for a handful of conditions, there are a multitude of causes that still engender
            severe visual disability.
            Additionally, many of these conditions are on the rise. This is only compounded by
            the fact that children born premature are
            now surviving at increasing rates, a wonderful trend that nevertheless increases the need for assistive
            technology, as retinopathy of prematurity is increasing significantly in this population. The most unsettling
of these facts are the associated increases in mortality and morbidity, which are sadly contributed
to by increases in suicide.
        </p>
        <p align="justify" style="font-size: 18px">
            &nbsp;&nbsp;&nbsp;&nbsp;Decreases in vision and sensory loss constrain one's mobility, and subsequently lead to the loss of
one's independence in activities of daily living (ADL) (i.e. eating, bathing, dressing, toileting and transferring),
            followed by issues with unemployment, quality of life losses, and functional dependencies
            that limit psychosocial wellbeing. The status quo currently offered to address the ADL needs
of those with visual impairment (VI) is to be aided by either individual supervisor who provides with
            personal guidance according to the VI's requirements, or special designed tools. For example, the most common assistive device
            for navigation and mobility of the low vision
is the cane. In addition, a package of gadgets and utensils with braille attached aim to prevent the
blind from injury in the perilous ADL like cutting or cooking.</p>
        <p align="justify" style="font-size: 18px">&nbsp;&nbsp;&nbsp;&nbsp;
            In our current era of versatile portable environmental sensors and artificial intelligence (AI) and computer
vision (CV) driven agent (i.e. self-driving vehicles), there is a clear need to move beyond current outdated
ADL assistant technology, with a paradigm shifting towards modern tools that will allow the visually impaired
            to regain the mobility losses associated with sensory deprivation, and stop the current downward
'spiral' of debility. To this end, the proposed wearable technology solution, Computer Vision for Activities
            of Daily Living through AI in Low Vision (CV 4 LV ), provides real-time personal security supervision
in one's immediate three-dimensional environment, and action planning/guidance in ADL, revitalizing the
virtual environmental perception as well as ADL independence of the LV. More specifically, we aim to 1)
design a wearable platform that is able to integrate multi-sensor fusion techniques to effectively combine
information obtained from the newly embedded infrared, ultrasound, and stereo-camera-based sensor systems
            (hardware) that are integrated into the novel smart service system, the CV 4 LV platform; 2) develop
algorithms that are able to perform functional scene understanding (i.e. semantic scene parsing, affordance
scene parsing), cyber-human interaction learning (i.e. decoding the relations between human activities and
objects in an unconstrained environment), and 3) support cyber-human interaction for ADL of the low
            vision (LV) by providing the assistive service operating at 'Default' mode or 'User-selective' mode to meet
different user requirements.
        </p>
    </td>
</tr>
<tr></tr>
</table>

<!-- Research Objectives -->
<hr>
<br>
<center><h1>Research Objectives</h1></center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
OBJECTIVE 1: Design the optimal Computer Vision for Activities of Daily Living through AI in Low Vi-
sion (CV 4 LV ) platform.</p>
        <p>
            OBJECTIVE 2: Design deep adversarial neural networks for functional scene understanding.
        </p>
        <p>
            OBJECTIVE 3: Design recurrent neural networks and deep reinforcement learning for cyber-human
            interaction understanding.
        </p>
        <p>
            OBJECTIVE 4: Design cyber-human interaction studies via CV 4 LV with ADL cases.
        </p>
    </td>
</tr>
<tr></tr>
</table>

<!-- Approaches -->
<br>
<hr>
<center><h1>Approaches</h1></center>
<br>
<table align=center width="1024px">
    <tr>
        <td>
            <center>
                <img src="./resources/7_2.png" width="900px"/><br>
            </center>
        </td>
    </tr>
</table>
<! -- Broader Impacts -->
<br>
<hr>
<center>
    <h1>Broader Impacts</h1>
</center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
           The results from this research will have broad impacts in the advancement of knowledge in the research
areas of assistive technology and low vision, human-computer interactions, computer vision, and machine
learning. The research results will be disseminated through publications in international conferences and
journals. In addition to Ph.D. students, we plan to involve BS, MS, and medical students in the research
project and advise on their senior design projects and thesis. The PIs will work closely with the Ph.D. students
            to develop the computer vision algorithms and the haptic/audio interfaces as described in the Research
Plan. The Ph.D. students will then work with the MS and BS students to implement the algorithms and
the haptic/audio interfaces and perform testing. As described above, there will be regular project meetings
between the PIs and students involved in the project. Students will also meet with individual PIs as often
as necessary to ensure that the parts they are working on are meeting targets. We will integrate student
training, course developments, laboratory experiments, outreach, and knowledge dissemination into our
proposed efforts. A project website (http://mmvc.poly.edu/cv4lv.html) has been set up along with
teaching modules focusing on various parts of the project to allow for knowledge transfer and outreach. We
will target community schools through various mechanisms including our existing STEM programs. NYU-
Tandon has significant activities in STEM efforts (including NSF sponsored projects) as well as extensive
outreach programs that involve high schools with minority and underrepresented groups.
        </p>
    </td>
</tr>
<tr></tr>
</table>


<!-- Credit -->
<hr>
<br>
<table style="font-size:14px" align=center>
    <tr>
        <td>
            This webpage template was borrowed from <a href='../../2020/MEPS'>MEPS</a>.
        </td>
    </tr>
</table>

</body>
</html>