<!DOCTYPE html>
<html lang="en">
<head>
    <title>Smart Assistive System</title>
    <meta property="og:title" content="Smart Assistive System"/>

    <link rel="stylesheet" type="text/css" href="css/style.css">
</head>

<body>

<!-- Title -->
<br>
<center>
    <span style="font-size:38px">Smart Assistive System for Visually Impaired Individuals: Display, Pointand Touch3D</span>
</center>


<!-- Organizations -->
<br>
<table align=center width=900px>
    <tr>
        <td align=center width=100px>
            <center>
                <span style="font-size:20px"><a href="http://mmvc.engineering.nyu.edu/">
                    NYU Multimedia and Visual Computing Lab, NYU Abu Dhabi and Tandon</a> </span>
            </center>
        </td>
    </tr>
</table>


<!-- Research Statement -->
<center><h1>Research Statement</h1></center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
           In the proposed research, we introduce a novel VI's use of sensory substitute principles
with proprioception to recover loss of visual sense, which is realized by the development of a unified frame-
work named PtTE through leveraging state-of-the-art techniques from multiple research domains including
deep learning, computer vision, robotics, rehabilitation engineering and low vision assistive technologies. It
enables the visually impaired (VI) to actively explore and engage with the surrounding scene with a user-
friendly interface. To achieve this goal, the proposed research will investigate two major research problems:
1) the development of blind-centered computer vision algorithms for surrounding scene understanding, and
2) the design of a wearable device to extend the VI's proprioception ability with a blind-friendly guidance
to help the VI freely localize and engage with their desired objects. In this project, three concrete research
thrusts are investigated: 1) to spatially map the surrounding scene objects/events in reference to the VI's
egocentric coordinate system, the project will develop a cam-glasses and wirstband based wearable solution
with a wide field view, a good range and an intuitive tactile-based human device interface, 2) to address
blind-centered scene understanding and interaction challenges, the project will study the curation of a blind-
centered visual object dataset, and develop computer vision algorithms that are based on hand attention and
utilize weak supervision to directly predict the object category the VI is pointing to, and 3) to facilitate the
blind-friendly human-environment interaction, the proposed project will design Point-to-Tell-and-Engage as
the system to provide a natural, intuitive and safe exploration of the surrounding environment using users'
fingertip.
        </p>
    </td>
</tr>
<tr></tr>
</table>


<!-- Research Objectives -->
<hr>
<br>
<center><h1>Research Objectives</h1></center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
            The research objective of this project is to advance the computer vision assisted technology for
the visually impaired (VI) to extend their use of sensory substitute principles by marrying proprioception
for intuitive user-environment interaction. The success of the project will enhance the technology to assist
the VI to actively explore, localize and engage with objects of interest with the surrounding environment
to obtain information by virtually 'pointing to' or 'engaging' certain activities (e.g. sit on a sofa, turn on
a light switch, etc) in their desired locations. To this end, we propose to develop a new wearable platform,
named Point-to-Tell-and-Engage, that will concentrate on the development of blind-centered: Device, Data,
Algorithms and System to enable the visually impaired to actively explore their surrounding scene by simply
moving their fingertips, and interact with the immediate environment under blind-friendly guidance with
minimized cognitive and motion load.
        </p>

    </td>
</tr>
<tr></tr>
</table>

<!-- Approaches -->
<br>
<hr>
<center><h1>Approaches</h1></center>
<br>
<table align=center width="1024px">
    <tr>
        <td>
            <center>
                <img src="./resources/4_3.png" width="900px"/><br>
            </center>
        </td>
    </tr>
    <tr>
        <td>
            <center>
                <span style="font-size:18px"></span>
            </center>
        </td>
    </tr>
    <tr>
        <td colspan='2'>
            <center>
                <img src="./resources/4_2.png" width="900px"/><br>
            </center>
        </td>
    </tr>
    <tr>
        <td colspan='2'>
            <center>
                <span style="font-size:18px"></span>
            </center>
        </td>
    </tr>
</table>
<! -- Broader Impacts -->
<br>
<hr>
<center>
    <h1>Broader Impacts</h1>
</center>
<table align=center width=900px>
<tr></tr>
<tr>
    <td width=600px>
        <br>
        <p align="justify" style="font-size: 18px">
           The success of the proposed project will lead to great social impact and commercial
impact and create positive impact to the under-represented group. For social impact, as many of these
mobility losses are also tied to quality of life compromises and an unemployment rate that approaches 60-
70% in almost every developed country in the moderately and severely visually impaired stratum. The
proposed platform will increase the safety profile of dynamic mobility in a myriad of environment. As the
barriers are lifted through use of our proposed platform, the overarching hypothesis is that the comorbidities
will abate and mortality statistics will ebb, saving healthcare dollars while offering significant humanitarian
upsides. We have received quite a few requests from governments (e.g. NYC Mayor's office for people with
disability (MOPD) and NYC department of transportation (DOT)) to provide safe and efficient travel of
pedestrians throughout New York City. In response to their requests, an ongoing project, named Cross-safe,
led by Dr. Fang and Rizzo, is the development of a smart wearable device to assist the visually impaired
travelers at signalized intersections. The PI plans to integrate the feature sets of the proposed platform to
Cross-Safe project, to help blind or visually impaired travelers who aim to cross roads safely and efficiently
given unpredictable traffic control. For commercial impact, the PI will explore the opportunities to transfer
the research output to commercial products so that it can benefit the general public. The PI plans to work
with NYU technology transfer office to commercialize the research outcome from this proposal to products
by licensing it to companies in both UAE and US. In addition, for positive impact to the under-represented
group, NYU Abu Dhabi and NYU New York have significant activities in STEM efforts including the
sponsored project as well as extensive outreach programs. Both PIs Fang and Rizzo commit their efforts on
the supervision of under-represented students by encouraging their participation in the proposed project. To
facilitate this effort, the PI plans to develop summer research programs that give students with the students
with visual impairments opportunities to participate the project in the human-centered, disability-focused
design of our Point-to-Tell-and-Engage system.
        </p>
    </td>
</tr>
<tr></tr>
</table>


<!-- Credit -->
<hr>
<br>
<table style="font-size:14px" align=center>
    <tr>
        <td>
            This webpage template was borrowed from <a href='../../2020/MEPS'>MEPS</a>.
        </td>
    </tr>
</table>

</body>
</html>